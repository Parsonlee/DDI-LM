{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.008378,"end_time":"2022-10-14T16:18:34.771499","exception":false,"start_time":"2022-10-14T16:18:34.763121","status":"completed"},"tags":[]},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-10T06:22:56.346521Z","iopub.status.busy":"2023-07-10T06:22:56.346187Z","iopub.status.idle":"2023-07-10T06:23:01.462550Z","shell.execute_reply":"2023-07-10T06:23:01.461562Z","shell.execute_reply.started":"2023-07-10T06:22:56.346488Z"},"papermill":{"duration":2.499314,"end_time":"2022-10-14T16:18:37.278066","exception":false,"start_time":"2022-10-14T16:18:34.778752","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import warnings\n","import random\n","import sqlite3\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import wandb\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    roc_auc_score,\n","    average_precision_score,\n",")\n","\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    BertConfig,\n","    BertForSequenceClassification,\n","    get_cosine_schedule_with_warmup,\n",")\n","\n","seed = 42\n","random.seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.006923,"end_time":"2022-10-14T16:18:37.293015","exception":false,"start_time":"2022-10-14T16:18:37.286092","status":"completed"},"tags":[]},"source":["# Hyperparamters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-10T06:23:01.465067Z","iopub.status.busy":"2023-07-10T06:23:01.463878Z","iopub.status.idle":"2023-07-10T06:23:01.508233Z","shell.execute_reply":"2023-07-10T06:23:01.507234Z","shell.execute_reply.started":"2023-07-10T06:23:01.465030Z"},"papermill":{"duration":0.076785,"end_time":"2022-10-14T16:18:37.376836","exception":false,"start_time":"2022-10-14T16:18:37.300051","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["file_path = \"./\"\n","\n","feature_list = [\"smile\", \"target\", \"enzyme\"]\n","device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","cfg = {\n","    \"model_name\": \"bert-base-cased\",\n","    # \"model_name\": \"microsoft/deberta-v3-base\",\n","    \"num_classes\": 100,\n","    \"epo_num\": 2,\n","    \"lr\": 5e-5,\n","    \"patience\": 0,  # 0 represent no early stop\n","    \"batch_size\": 16,\n","    \"max_len\": 256,\n","    \"use_amp\": False,  # Cause gradient underflow -> NaN\n","    \"forzen\": False,\n","}"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.006929,"end_time":"2022-10-14T16:18:37.493037","exception":false,"start_time":"2022-10-14T16:18:37.486108","status":"completed"},"tags":[]},"source":["# Data"]},{"cell_type":"markdown","metadata":{},"source":["#### large dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-10T06:23:01.571067Z","iopub.status.busy":"2023-07-10T06:23:01.570722Z","iopub.status.idle":"2023-07-10T06:23:01.898430Z","shell.execute_reply":"2023-07-10T06:23:01.897513Z","shell.execute_reply.started":"2023-07-10T06:23:01.571036Z"},"trusted":true},"outputs":[],"source":["events = pd.read_csv(f\"{file_path}/data/events.csv\", index_col=0)\n","df_drug = pd.read_csv(f\"{file_path}/data/drugs.csv\", index_col=0)\n","# events = pd.read_csv(f'{file_path}/events.csv', index_col=0)\n","# df_drug = pd.read_csv(f'{file_path}/drugs.csv', index_col=0)\n","\n","# events.shape, df_drug.shape\n","display(events.head(1))\n","display(df_drug.head(1))"]},{"cell_type":"markdown","metadata":{},"source":["#### small dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# conn = sqlite3.connect(f\"{file_path}/event.db\")\n","# df_drug = pd.read_sql(\"select * from drug;\", conn)\n","# extraction = pd.read_sql(\"select * from extraction;\", conn)\n","# mechanism = extraction[\"mechanism\"]\n","# action = extraction[\"action\"]\n","# drugA = extraction[\"drugA\"]\n","# drugB = extraction[\"drugB\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# extraction[\"label_text\"] = extraction.mechanism + \" \" + extraction.action\n","\n","# extraction[\"label\"] = LabelEncoder().fit_transform(extraction[\"label_text\"])\n","# extraction[\"label_text\"] = extraction[\"label_text\"].apply(str.lower)\n","# extraction = extraction.drop([\"index\"], axis=1)\n","# df_drug = df_drug.drop([\"id\", \"index\", \"pathway\"], axis=1)\n","# df_drug = df_drug.set_index(\"name\")\n","\n","# # ## check number of classes\n","# # extraction['label'].nunique()\n","# # ## check number of drugs\n","# # # df_drug.index\n","\n","# display(df_drug.head(2))\n","# display(extraction.head(2))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.00686,"end_time":"2022-10-14T16:18:37.666693","exception":false,"start_time":"2022-10-14T16:18:37.659833","status":"completed"},"tags":[]},"source":["# Preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DDI_Dataset(Dataset):\n","    def __init__(self, ev_df, drug_df, tokenizer, max_len=256):\n","        self.events = ev_df\n","        self.drugs = drug_df\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return self.events.shape[0]\n","\n","    def __getitem__(self, index):\n","        d_a, d_b, labels = self.events.iloc[index, [2, 3, -1]]\n","\n","        ## without prompt\n","        # d_a_seq = d_a + \",\" + ','.join(self.drugs.loc[d_a].values)\n","        # d_b_seq = d_b + \",\" + ','.join(self.drugs.loc[d_b].values)\n","\n","        ## use different modal\n","        # modal = 'smile'\n","        # d_a_seq = d_a + f\", the drug {d_a}'s {modal} information is: \" + self.drugs.loc[d_a].smile\n","        # d_b_seq = d_b + f\", the drug {d_b}'s {modal} information is: \" + self.drugs.loc[d_b].smile\n","\n","        # text = f'{d_a_seq + \" \" + self.tokenizer.sep_token + \" \" + d_b_seq}'\n","\n","        ## use prompt\n","        text = f\"The drug {d_a} interacts with the drug {d_b}. \\\n","        The drug {d_a}'s information is: {', '.join(self.drugs.loc[d_a].values)}. \\\n","        The drug {d_b}'s information is: {', '.join(self.drugs.loc[d_b].values)}.\"\n","\n","        # print(text)\n","\n","        encode_dict = self.tokenizer.encode_plus(\n","            text=text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_attention_mask=True,\n","            return_tensors=\"pt\",\n","        )\n","        ids = encode_dict[\"input_ids\"].squeeze(0)\n","        masks = encode_dict[\"attention_mask\"].squeeze(0)\n","\n","        return {\"ids\": ids, \"masks\": masks, \"labels\": labels}"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007085,"end_time":"2022-10-14T16:18:47.067096","exception":false,"start_time":"2022-10-14T16:18:47.060011","status":"completed"},"tags":[]},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-10T06:23:53.685490Z","iopub.status.busy":"2023-07-10T06:23:53.684888Z","iopub.status.idle":"2023-07-10T06:23:57.380751Z","shell.execute_reply":"2023-07-10T06:23:57.379828Z","shell.execute_reply.started":"2023-07-10T06:23:53.685450Z"},"trusted":true},"outputs":[],"source":["model_name = cfg[\"model_name\"]\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name, num_labels=cfg[\"num_classes\"]\n",")\n","\n","\n","# train from scratch\n","# config = BertConfig()\n","# model = BertForSequenceClassification(config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FocalLoss(nn.Module):\n","    def __init__(self, num_classes, alpha=0.25, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, logits, targets):\n","        probs = F.softmax(logits, dim=1)\n","        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes)\n","        pt = torch.sum(probs * targets_one_hot, dim=1) + 1e-6\n","        focal_loss = -self.alpha * (1 - pt) ** self.gamma * torch.log(pt)\n","        return focal_loss.mean()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007079,"end_time":"2022-10-14T16:18:47.275069","exception":false,"start_time":"2022-10-14T16:18:47.267990","status":"completed"},"tags":[]},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# wandb_key = \"****\"\n","# wandb.login(key=wandb_key)\n","\n","run = wandb.init(\n","    project=\"DDI\",\n","    config=cfg,\n","    dir=f\"{file_path}\",\n","    name=\"w/ each\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import label_binarize\n","\n","\n","def evaluate_metrics(pred_probs, labels):\n","    pred_probs = np.concatenate(pred_probs, axis=0)\n","    labels = np.concatenate(labels, axis=0)\n","    print(f\"evaluate: {pred_probs.shape}, {labels.shape}\")\n","    assert pred_probs.shape[0] == labels.shape[0]\n","\n","    # 获得预测的类别\n","    predicted_labels = pred_probs.argmax(axis=1)\n","\n","    # 计算accuracy\n","    accuracy = accuracy_score(labels, predicted_labels)\n","\n","    # 计算precision、recall、F1-score\n","    precision_mi = precision_score(labels, predicted_labels, average=\"micro\")\n","    recall_mi = recall_score(labels, predicted_labels, average=\"micro\")\n","    f1_mi = f1_score(labels, predicted_labels, average=\"micro\")\n","    precision_ma = precision_score(labels, predicted_labels, average=\"macro\")\n","    recall_ma = recall_score(labels, predicted_labels, average=\"macro\")\n","    f1_ma = f1_score(labels, predicted_labels, average=\"macro\")\n","\n","    # 计算AUC和AUPR\n","    auc_score = roc_auc_score(labels, pred_probs, average=\"macro\", multi_class=\"ovr\")\n","\n","    # 初始化一个列表来存储每个类别的AUPR分数\n","    aupr_scores = []\n","    # 对每个类别计算AUPR\n","    for class_idx in range(pred_probs.shape[1]):\n","        class_labels = (labels == class_idx).astype(int)\n","        class_probs = pred_probs[:, class_idx]\n","        aupr = average_precision_score(class_labels, class_probs)\n","        aupr_scores.append(aupr)\n","    # 平均AUPR分数\n","    average_aupr = sum(aupr_scores) / len(aupr_scores)\n","\n","    each_eval_type = 6\n","    result_eve = np.zeros((100, each_eval_type), dtype=float)\n","    y_one_hot = label_binarize(labels, classes=range(100))\n","    pred_one_hot = label_binarize(predicted_labels, classes=range(100))\n","    for i in range(100):\n","        result_eve[i, 0] = accuracy_score(\n","            y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel()\n","        )\n","        result_eve[i, 1] = average_precision_score(\n","            y_one_hot.take([i], axis=1).ravel(),\n","            pred_one_hot.take([i], axis=1).ravel(),\n","            average=None,\n","        )\n","        result_eve[i, 2] = roc_auc_score(\n","            y_one_hot.take([i], axis=1).ravel(),\n","            pred_one_hot.take([i], axis=1).ravel(),\n","            average=None,\n","        )\n","        result_eve[i, 3] = f1_score(\n","            y_one_hot.take([i], axis=1).ravel(),\n","            pred_one_hot.take([i], axis=1).ravel(),\n","            average=\"binary\",\n","        )\n","        result_eve[i, 4] = precision_score(\n","            y_one_hot.take([i], axis=1).ravel(),\n","            pred_one_hot.take([i], axis=1).ravel(),\n","            average=\"binary\",\n","        )\n","        result_eve[i, 5] = recall_score(\n","            y_one_hot.take([i], axis=1).ravel(),\n","            pred_one_hot.take([i], axis=1).ravel(),\n","            average=\"binary\",\n","        )\n","\n","    result_all = {\n","        \"accuracy\": accuracy,\n","        \"precision_micro\": precision_mi,\n","        \"precision_macro\": precision_ma,\n","        \"recall_micro\": recall_mi,\n","        \"recall_macro\": recall_ma,\n","        \"f1_micro\": f1_mi,\n","        \"f1_macro\": f1_ma,\n","        \"auc_score\": auc_score,\n","        \"aupr_score\": average_aupr,\n","    }\n","    return result_all, result_eve"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-10T06:23:57.411238Z","iopub.status.busy":"2023-07-10T06:23:57.410885Z","iopub.status.idle":"2023-07-10T06:23:57.428581Z","shell.execute_reply":"2023-07-10T06:23:57.427617Z","shell.execute_reply.started":"2023-07-10T06:23:57.411204Z"},"trusted":true},"outputs":[],"source":["# Training configuration\n","\n","optimizer = optim.AdamW(model.parameters(), lr=cfg[\"lr\"], eps=1e-3)\n","scaler = torch.cuda.amp.GradScaler(enabled=cfg[\"use_amp\"])\n","if cfg[\"use_amp\"]:\n","    print(\"Using AMP!\")\n","\n","if cfg[\"forzen\"]:\n","    print(\"Freeze the base model!\")\n","    for param in model.base_model.parameters():\n","        param.requires_grad = False\n","\n","\n","criterion = FocalLoss(num_classes=cfg[\"num_classes\"])\n","# criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-10T06:23:57.431957Z","iopub.status.busy":"2023-07-10T06:23:57.431267Z"},"trusted":true},"outputs":[],"source":["def train_fn(model, train_loader, test_loader):\n","    # Start training\n","\n","    num_training_steps = cfg[\"epo_num\"] * len(train_loader)\n","    num_warmup_steps = int(0.3 * num_training_steps)\n","    lr_scheduler = get_cosine_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=num_training_steps,\n","    )\n","\n","    for i in range(cfg[\"epo_num\"]):\n","        print(\"Epoch {}/{}\".format(i + 1, cfg[\"epo_num\"]))\n","        print(\"-\" * 10)\n","\n","        train_epoch_loss = []\n","        valid_epoch_loss = []\n","\n","        model.to(device)\n","        model.train()\n","        for step, batch in tqdm(\n","            enumerate(train_loader), total=len(train_loader), desc=\"Train\"\n","        ):\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            ids, masks, labels = batch[\"ids\"], batch[\"masks\"], batch[\"labels\"]\n","\n","            with torch.autocast(\n","                device_type=\"cuda\", dtype=torch.float16, enabled=cfg[\"use_amp\"]\n","            ):\n","                outputs = model(ids, masks)\n","                logits = outputs.logits\n","                loss = criterion(logits, labels)\n","\n","            train_epoch_loss.append(loss.item())\n","\n","            if step % 200 == 0:\n","                wandb.log({\"train_loss\": loss.item()})\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            lr_scheduler.step()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        total_probs = []\n","        total_labels = []\n","        model.eval()\n","        with torch.no_grad():\n","            for step, b in tqdm(\n","                enumerate(test_loader), total=len(test_loader), desc=\"Valid\"\n","            ):\n","                b = {k: v.to(device) for k, v in b.items()}\n","                ids, masks, labels = b[\"ids\"], b[\"masks\"], b[\"labels\"]\n","\n","                outputs = model(ids, masks)\n","                logits = outputs.logits\n","                loss = criterion(logits, labels)\n","\n","                valid_epoch_loss.append(loss.item())\n","\n","                probs = F.softmax(logits, dim=1).cpu().numpy()\n","                total_probs.append(probs)\n","                total_labels.append(labels.cpu().numpy())\n","\n","                if step % 200 == 0:\n","                    wandb.log({\"valid_loss\": loss.item()})\n","\n","        total_probs = np.concatenate(total_probs, axis=0)\n","        total_labels = np.concatenate(total_labels, axis=0)\n","\n","        avg_train_loss = sum(train_epoch_loss) / len(train_epoch_loss)\n","        avg_valid_loss = sum(valid_epoch_loss) / len(valid_epoch_loss)\n","\n","        print(\"Training Loss: {:.4f}\".format(avg_train_loss))\n","        print(\"Valid Loss: {:.4f}\".format(avg_valid_loss))\n","        print()\n","\n","    return total_probs, total_labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cross_val(events, df_drugs, tokenizer, model):\n","    skf = StratifiedKFold(n_splits=5)\n","    fold = 0\n","    total_pred_scores = []\n","    total_labels = []\n","\n","    for train_index, test_index in skf.split(np.zeros(len(events)), events[\"label\"]):\n","        fold += 1\n","        print(f\"Training fold {fold} start!\")\n","\n","        train_dataset = DDI_Dataset(\n","            events.iloc[train_index], df_drugs, tokenizer, max_len=cfg[\"max_len\"]\n","        )\n","        train_loader = DataLoader(\n","            train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True\n","        )\n","        test_dataset = DDI_Dataset(\n","            events.iloc[test_index], df_drugs, tokenizer, max_len=cfg[\"max_len\"]\n","        )\n","        test_loader = DataLoader(\n","            test_dataset, batch_size=cfg[\"batch_size\"], shuffle=False\n","        )\n","\n","        pred_scores, labels = train_fn(model, train_loader, test_loader)\n","\n","        total_pred_scores.append(pred_scores)\n","        total_labels.append(labels)\n","        break\n","\n","    cv_results = evaluate_metrics(total_pred_scores, total_labels)\n","    print(\"results: \", cv_results)\n","\n","    return cv_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_results = cross_val(events, df_drug, tokenizer, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model_save_directory = f\"{file_path}/saved_models/{cfg['model_name']}-pretrained\"\n","# tokenizer.save_pretrained(model_save_directory)\n","# model.save_pretrained(model_save_directory)\n","# wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"work","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":4}
